{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import timm\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import torch_optimizer as optimizer\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "from config import settings\n",
    "\n",
    "import models.spinalnet_resnet as spinalnet_resnet\n",
    "import models.effnet as effnet\n",
    "import models.densenet as densenet\n",
    "import models.spinalnet_vgg as spinalnet_vgg\n",
    "import models.vitL16 as vitL16\n",
    "import models.alexnet_vgg as alexnet_vgg\n",
    "import models.resnet18 as resnet18\n",
    "\n",
    "import  data\n",
    "# import data.segmentation as segmentation\n",
    "# import metrics.metrics as metrics\n",
    "from data import DataPart\n",
    "from train import Trainer\n",
    "import metrics\n",
    "\n",
    "\n",
    "all_models = [\n",
    "    ('ResNet18', resnet18),\n",
    "    ('EfficientNet', effnet),\n",
    "    # ('DenseNet', densenet),\n",
    "    # ('SpinalNet_ResNet', spinalnet_resnet),\n",
    "    # ('SpinalNet_VGG', spinalnet_vgg),\n",
    "    # ('ViTL16', vitL16),\n",
    "    # ('AlexNet_VGG', alexnet_vgg)\n",
    "]\n",
    "\n",
    "all_optimizers = [\n",
    "    ('SGD', optim.SGD),\n",
    "    ('Rprop', optim.Rprop),\n",
    "    ('Adam', optim.Adam),\n",
    "    ('NAdam', optim.NAdam),\n",
    "    ('RAdam', optim.RAdam),\n",
    "    ('AdamW', optim.AdamW),\n",
    "    #('Adagrad', optim.Adagrad),\n",
    "    ('RMSprop', optim.RMSprop),\n",
    "    #('Adadelta', optim.Adadelta),\n",
    "    ('DiffGrad', optimizer.DiffGrad),\n",
    "    # ('LBFGS', optim.LBFGS)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mszekhov/Desktop/current_projects/galaxyHackers/venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "252\n",
      "84\n",
      "84\n",
      "244\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "datasets, dataloaders = data.create_dataloaders()\n",
    "\n",
    "train_loader = dataloaders[DataPart.TRAIN]\n",
    "val_loader = dataloaders[DataPart.VALIDATE]\n",
    "test_loader = dataloaders[DataPart.TEST_DR5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parser = argparse.ArgumentParser(description='Model training')\n",
    "# parser.add_argument('--models', nargs='+', default=['ResNet18', 'EfficientNet', 'DenseNet', 'SpinalNet_ResNet', 'SpinalNet_VGG', 'ViTL16', 'AlexNet_VGG'],\n",
    "#                     help='List of models to train (default: all)')\n",
    "# parser.add_argument('--epochs', type=int, default=5, help='Number of epochs to train (default: 5)')\n",
    "# parser.add_argument('--lr', type=float, default=0.0001, help='Learning rate for optimizer (default: 0.0001)')\n",
    "# parser.add_argument('--mm', type=float, default=0.9, help='Momentum for optimizer (default: 0.9)')\n",
    "# parser.add_argument('--optimizer', choices=[name for name, _ in all_optimizers], default='Adam', help='Optimizer to use (default: Adam)')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# selected_models = [(model_name, model) for model_name, model in models if model_name in args.models]\n",
    "\n",
    "# num_epochs = args.epochs\n",
    "# lr = args.lr\n",
    "# momentum = args.mm\n",
    "# optimizer_name = args.optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_models = all_models[:2]\n",
    "\n",
    "num_epochs = 1\n",
    "lr = 0.0001\n",
    "momentum = 0.9\n",
    "optimizer_name = \"Adam\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzehov1\u001b[0m (\u001b[33mmzekhov\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/mszekhov/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mszekhov/Desktop/current_projects/galaxyHackers/wandb/run-20240825_190256-l2a5mw4i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mzekhov/cluster-search/runs/l2a5mw4i' target=\"_blank\">cool-paper-53</a></strong> to <a href='https://wandb.ai/mzekhov/cluster-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mzekhov/cluster-search' target=\"_blank\">https://wandb.ai/mzekhov/cluster-search</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mzekhov/cluster-search/runs/l2a5mw4i' target=\"_blank\">https://wandb.ai/mzekhov/cluster-search/runs/l2a5mw4i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if settings.wandb_api_token:\n",
    "    wandb.login(key=settings.wandb_api_token)\n",
    "    wandb.init(project='cluster-search', config={}, reinit=True)\n",
    "else:\n",
    "    wandb.init(project='cluster-search', config={}, reinit=True)\n",
    "\n",
    "\n",
    "wandb.config.models = [name for name, _ in selected_models]\n",
    "wandb.config.num_epochs = num_epochs\n",
    "wandb.config.lr = lr\n",
    "wandb.config.momentum = momentum\n",
    "wandb.config.optimizer = optimizer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "results = {}\n",
    "val_results = {}\n",
    "\n",
    "classes = ('random', 'clusters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.68s/batch]      | 0/1 [00:00<?, ?epoch/s]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.19s/it]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.39s/it]                                          \n",
      "100%|██████████| 1/1 [00:32<00:00, 32.63s/batch]          | 0/1 [00:00<?, ?epoch/s]\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.13s/it]\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.49s/it]                                              \n"
     ]
    }
   ],
   "source": [
    "for model_name, model in selected_models:\n",
    "\n",
    "     model = model.load_model()\n",
    "\n",
    "     optimizer_class = dict(all_optimizers)[optimizer_name]\n",
    "\n",
    "     if optimizer_name in ['SGD', 'RMSprop']:\n",
    "          optimizer = optimizer_class(model.parameters(), lr=lr, momentum=momentum) \n",
    "     else:\n",
    "          optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "         \n",
    "     trainer = Trainer(\n",
    "          model=model,\n",
    "          criterion=criterion,\n",
    "          optimizer=optimizer,\n",
    "          train_dataloader=train_loader,\n",
    "          val_dataloader=val_loader,\n",
    "\n",
    "     )\n",
    "\n",
    "     trainer.train(num_epochs)\n",
    "\n",
    "     for step in range(trainer.global_step):\n",
    "          wandb.log(\n",
    "               {\n",
    "                    f'{model_name}_{optimizer_name}_train_loss': trainer.history['train_loss'][step], \n",
    "                    f'{model_name}_{optimizer_name}_train_accuracy':trainer.history['train_acc'][step], \n",
    "                    'global_step': step + 1})\n",
    "          \n",
    "     for epoch in range(num_epochs):\n",
    "          wandb.log(\n",
    "               {\n",
    "                    f'{model_name}_{optimizer_name}_val_loss': trainer.history['val_loss'][epoch], \n",
    "                    f'{model_name}_{optimizer_name}_val_accuracy': trainer.history['val_acc'][epoch], \n",
    "                    'epoch': epoch})\n",
    "\n",
    "     \n",
    "     train_table = wandb.Table(\n",
    "          data=[\n",
    "               [\n",
    "                    step, \n",
    "                    trainer.history['train_loss'][step], \n",
    "                    trainer.history['train_acc'][step]\n",
    "               ] for step in range(trainer.global_step)],\n",
    "          columns=[\"Epoch\", \"Loss\", \"Accuracy\"])\n",
    "\n",
    "     val_table = wandb.Table(\n",
    "          data=[\n",
    "               [\n",
    "                    epoch, \n",
    "                    trainer.history['val_loss'][epoch], \n",
    "                    trainer.history['val_acc'][epoch]\n",
    "               ] for epoch in range(num_epochs)],\n",
    "          columns=[\"Epoch\", \"Loss\", \"Accuracy\"])\n",
    "\n",
    "     wandb.log({\"Train Metrics\": train_table, \"Validation Metrics\": val_table})\n",
    "\n",
    "     y_pred, y_probs, y_true, *_ = trainer.test(test_loader)\n",
    "\n",
    "     metrics.modelPerformance(model_name, optimizer_name, y_true, y_pred, y_probs, classes)\n",
    "\n",
    "metrics.combine_metrics(selected_models, optimizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a965685e834803a28208ffff926c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.009 MB uploaded\\r'), FloatProgress(value=0.4286352967475131, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ResNet18_Adam_train_accuracy</td><td>▆▆▅▆▆▆▇▇▇█▇▇▇▇█▁</td></tr><tr><td>ResNet18_Adam_train_loss</td><td>█▇█▇▇▇▅▅▅▄▆▅▃▂▁▇</td></tr><tr><td>ResNet18_Adam_val_accuracy</td><td>▁</td></tr><tr><td>ResNet18_Adam_val_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>global_step</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ResNet18_Adam_train_accuracy</td><td>0.0</td></tr><tr><td>ResNet18_Adam_train_loss</td><td>0.6936</td></tr><tr><td>ResNet18_Adam_val_accuracy</td><td>0.72852</td></tr><tr><td>ResNet18_Adam_val_loss</td><td>0.62251</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>global_step</td><td>16</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">blooming-mountain-47</strong> at: <a href='https://wandb.ai/mzekhov/cluster-search/runs/wgcclber' target=\"_blank\">https://wandb.ai/mzekhov/cluster-search/runs/wgcclber</a><br/> View project at: <a href='https://wandb.ai/mzekhov/cluster-search' target=\"_blank\">https://wandb.ai/mzekhov/cluster-search</a><br/>Synced 5 W&B file(s), 2 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240816_223412-wgcclber/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No wandb run found.\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()\n",
    "\n",
    "wandb_run = wandb.run\n",
    "if wandb_run:\n",
    "    logged_metrics = wandb_run.history()\n",
    "    print(\"Logged Metrics:\")\n",
    "    for key, value in logged_metrics.items():\n",
    "        print(key, \":\", value)\n",
    "else:\n",
    "    print(\"No wandb run found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mszekhov/Desktop/current_projects/galaxyHackers/segmentation.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model = torch.load(weights_path, map_location=device)\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:19<00:00,  2.79s/it]\n",
      "100%|██████████| 7/7 [00:20<00:00,  2.98s/it]\n",
      "100%|██████████| 7/7 [00:21<00:00,  3.12s/it]\n",
      "100%|██████████| 7/7 [00:20<00:00,  2.91s/it]\n",
      "100%|██████████| 7/7 [00:19<00:00,  2.85s/it]\n",
      "100%|██████████| 7/7 [00:20<00:00,  2.98s/it]\n",
      "100%|██████████| 7/7 [00:19<00:00,  2.81s/it]\n",
      "100%|██████████| 7/7 [00:20<00:00,  2.90s/it]\n",
      "100%|██████████| 7/7 [00:22<00:00,  3.25s/it]\n",
      "100%|██████████| 7/7 [00:22<00:00,  3.23s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:18<00:00,  2.66s/it]\n",
      "100%|██████████| 7/7 [00:19<00:00,  2.84s/it]\n",
      "100%|██████████| 7/7 [00:19<00:00,  2.78s/it]\n",
      "100%|██████████| 7/7 [00:28<00:00,  4.08s/it]\n",
      "100%|██████████| 7/7 [00:18<00:00,  2.62s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:18<00:00,  2.66s/it]\n",
      "100%|██████████| 7/7 [00:22<00:00,  3.25s/it]\n",
      "100%|██████████| 7/7 [00:24<00:00,  3.56s/it]\n",
      "100%|██████████| 7/7 [00:25<00:00,  3.66s/it]\n",
      "100%|██████████| 7/7 [00:26<00:00,  3.74s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:50<00:00,  3.13s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 34.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "Failed attempt 0 to download /Users/mszekhov/Desktop/current_projects/galaxyHackers/storage/segmentation/samples/dr5_big/758/679.fits with an HTTPError\n",
      "Failed attempt 0 to download /Users/mszekhov/Desktop/current_projects/galaxyHackers/storage/segmentation/samples/dr5_big/758/728.fits with an HTTPError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:43<00:00,  2.74s/it]\n"
     ]
    }
   ],
   "source": [
    "import segmentation\n",
    "\n",
    "model_name, model = selected_models[0]\n",
    "segmentation.create_segmentation_plots(\n",
    "    model,\n",
    "    model_name,\n",
    "    optimizer_name=optimizer_name\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
