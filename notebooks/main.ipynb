{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import timm\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import torch_optimizer as optimizer\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "from config import settings\n",
    "\n",
    "import models.spinalnet_resnet as spinalnet_resnet\n",
    "import models.effnet as effnet\n",
    "import models.densenet as densenet\n",
    "import models.spinalnet_vgg as spinalnet_vgg\n",
    "import models.vitL16 as vitL16\n",
    "import models.alexnet_vgg as alexnet_vgg\n",
    "import models.resnet18 as resnet18\n",
    "\n",
    "import  data\n",
    "# import data.segmentation as segmentation\n",
    "# import metrics.metrics as metrics\n",
    "from data import DataPart\n",
    "from train import Trainer\n",
    "import metrics\n",
    "\n",
    "\n",
    "all_models = [\n",
    "    ('ResNet18', resnet18),\n",
    "    ('EfficientNet', effnet),\n",
    "    # ('DenseNet', densenet),\n",
    "    # ('SpinalNet_ResNet', spinalnet_resnet),\n",
    "    # ('SpinalNet_VGG', spinalnet_vgg),\n",
    "    # ('ViTL16', vitL16),\n",
    "    # ('AlexNet_VGG', alexnet_vgg)\n",
    "]\n",
    "\n",
    "all_optimizers = [\n",
    "    ('SGD', optim.SGD),\n",
    "    ('Rprop', optim.Rprop),\n",
    "    ('Adam', optim.Adam),\n",
    "    ('NAdam', optim.NAdam),\n",
    "    ('RAdam', optim.RAdam),\n",
    "    ('AdamW', optim.AdamW),\n",
    "    #('Adagrad', optim.Adagrad),\n",
    "    ('RMSprop', optim.RMSprop),\n",
    "    #('Adadelta', optim.Adadelta),\n",
    "    ('DiffGrad', optimizer.DiffGrad),\n",
    "    # ('LBFGS', optim.LBFGS)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mszekhov/Desktop/current_projects/galaxyHackers/venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "252\n",
      "84\n",
      "84\n",
      "244\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "datasets, dataloaders = data.create_dataloaders()\n",
    "\n",
    "train_loader = dataloaders[DataPart.TRAIN]\n",
    "val_loader = dataloaders[DataPart.VALIDATE]\n",
    "test_loader = dataloaders[DataPart.TEST_DR5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parser = argparse.ArgumentParser(description='Model training')\n",
    "# parser.add_argument('--models', nargs='+', default=['ResNet18', 'EfficientNet', 'DenseNet', 'SpinalNet_ResNet', 'SpinalNet_VGG', 'ViTL16', 'AlexNet_VGG'],\n",
    "#                     help='List of models to train (default: all)')\n",
    "# parser.add_argument('--epochs', type=int, default=5, help='Number of epochs to train (default: 5)')\n",
    "# parser.add_argument('--lr', type=float, default=0.0001, help='Learning rate for optimizer (default: 0.0001)')\n",
    "# parser.add_argument('--mm', type=float, default=0.9, help='Momentum for optimizer (default: 0.9)')\n",
    "# parser.add_argument('--optimizer', choices=[name for name, _ in all_optimizers], default='Adam', help='Optimizer to use (default: Adam)')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# selected_models = [(model_name, model) for model_name, model in models if model_name in args.models]\n",
    "\n",
    "# num_epochs = args.epochs\n",
    "# lr = args.lr\n",
    "# momentum = args.mm\n",
    "# optimizer_name = args.optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_models = all_models[:2]\n",
    "\n",
    "num_epochs = 1\n",
    "lr = 0.0001\n",
    "momentum = 0.9\n",
    "optimizer_name = \"Adam\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzehov1\u001b[0m (\u001b[33mmzekhov\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/mszekhov/.netrc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ffe62e664e466f8ca2847f0b06a5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011168349077828073, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mszekhov/Desktop/current_projects/galaxyHackers/wandb/run-20240828_132310-jzwtfvaw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mzekhov/cluster-search/runs/jzwtfvaw' target=\"_blank\">divine-oath-81</a></strong> to <a href='https://wandb.ai/mzekhov/cluster-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mzekhov/cluster-search' target=\"_blank\">https://wandb.ai/mzekhov/cluster-search</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mzekhov/cluster-search/runs/jzwtfvaw' target=\"_blank\">https://wandb.ai/mzekhov/cluster-search/runs/jzwtfvaw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if settings.wandb_api_token:\n",
    "    wandb.login(key=settings.wandb_api_token)\n",
    "    wandb.init(project='cluster-search', config={}, reinit=True)\n",
    "else:\n",
    "    wandb.init(project='cluster-search', config={}, reinit=True)\n",
    "\n",
    "\n",
    "wandb.config.models = [name for name, _ in selected_models]\n",
    "wandb.config.num_epochs = num_epochs\n",
    "wandb.config.lr = lr\n",
    "wandb.config.momentum = momentum\n",
    "wandb.config.optimizer = optimizer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "results = {}\n",
    "val_results = {}\n",
    "\n",
    "classes = ('random', 'clusters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.69s/batch]      | 0/1 [00:00<?, ?epoch/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.35s/it]                                          \n",
      "/Users/mszekhov/Desktop/current_projects/galaxyHackers/metrics.py:144: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  recall_per_bin = red_shift_predictions.groupby('bucket').apply(lambda x: recall_score(x['y_true'], x['y_pred']))\n",
      "/Users/mszekhov/Desktop/current_projects/galaxyHackers/metrics.py:147: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  proportions = red_shift_predictions.groupby('bucket')['red_shift_type'].value_counts(normalize=True).unstack().fillna(0)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 62\u001b[0m\n\u001b[1;32m     58\u001b[0m      predictions, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtest(test_loader)\n\u001b[1;32m     60\u001b[0m      metrics\u001b[38;5;241m.\u001b[39mmodelPerformance(model_name, optimizer_name, predictions, classes)\n\u001b[0;32m---> 62\u001b[0m      \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     65\u001b[0m metrics\u001b[38;5;241m.\u001b[39mcombine_metrics(selected_models, optimizer_name)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "for model_name, model in selected_models:\n",
    "\n",
    "     model = model.load_model()\n",
    "\n",
    "     optimizer_class = dict(all_optimizers)[optimizer_name]\n",
    "\n",
    "     if optimizer_name in ['SGD', 'RMSprop']:\n",
    "          optimizer = optimizer_class(model.parameters(), lr=lr, momentum=momentum) \n",
    "     else:\n",
    "          optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "         \n",
    "     trainer = Trainer(\n",
    "          model=model,\n",
    "          criterion=criterion,\n",
    "          optimizer=optimizer,\n",
    "          train_dataloader=train_loader,\n",
    "          val_dataloader=val_loader,\n",
    "\n",
    "     )\n",
    "\n",
    "     trainer.train(num_epochs)\n",
    "\n",
    "     for step in range(trainer.global_step):\n",
    "          wandb.log(\n",
    "               {\n",
    "                    f'{model_name}_{optimizer_name}_train_loss': trainer.history['train_loss'][step], \n",
    "                    f'{model_name}_{optimizer_name}_train_accuracy':trainer.history['train_acc'][step], \n",
    "                    'global_step': step + 1})\n",
    "          \n",
    "     for epoch in range(num_epochs):\n",
    "          wandb.log(\n",
    "               {\n",
    "                    f'{model_name}_{optimizer_name}_val_loss': trainer.history['val_loss'][epoch], \n",
    "                    f'{model_name}_{optimizer_name}_val_accuracy': trainer.history['val_acc'][epoch], \n",
    "                    'epoch': epoch})\n",
    "\n",
    "     \n",
    "     train_table = wandb.Table(\n",
    "          data=[\n",
    "               [\n",
    "                    step, \n",
    "                    trainer.history['train_loss'][step], \n",
    "                    trainer.history['train_acc'][step]\n",
    "               ] for step in range(trainer.global_step)],\n",
    "          columns=[\"Epoch\", \"Loss\", \"Accuracy\"])\n",
    "\n",
    "     val_table = wandb.Table(\n",
    "          data=[\n",
    "               [\n",
    "                    epoch, \n",
    "                    trainer.history['val_loss'][epoch], \n",
    "                    trainer.history['val_acc'][epoch]\n",
    "               ] for epoch in range(num_epochs)],\n",
    "          columns=[\"Epoch\", \"Loss\", \"Accuracy\"])\n",
    "\n",
    "     wandb.log({\"Train Metrics\": train_table, \"Validation Metrics\": val_table})\n",
    "\n",
    "     predictions, *_ = trainer.test(test_loader)\n",
    "\n",
    "     metrics.modelPerformance(model_name, optimizer_name, predictions, classes)\n",
    "\n",
    "     raise\n",
    "\n",
    "\n",
    "metrics.combine_metrics(selected_models, optimizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a965685e834803a28208ffff926c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.009 MB uploaded\\r'), FloatProgress(value=0.4286352967475131, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ResNet18_Adam_train_accuracy</td><td>▆▆▅▆▆▆▇▇▇█▇▇▇▇█▁</td></tr><tr><td>ResNet18_Adam_train_loss</td><td>█▇█▇▇▇▅▅▅▄▆▅▃▂▁▇</td></tr><tr><td>ResNet18_Adam_val_accuracy</td><td>▁</td></tr><tr><td>ResNet18_Adam_val_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>global_step</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ResNet18_Adam_train_accuracy</td><td>0.0</td></tr><tr><td>ResNet18_Adam_train_loss</td><td>0.6936</td></tr><tr><td>ResNet18_Adam_val_accuracy</td><td>0.72852</td></tr><tr><td>ResNet18_Adam_val_loss</td><td>0.62251</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>global_step</td><td>16</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">blooming-mountain-47</strong> at: <a href='https://wandb.ai/mzekhov/cluster-search/runs/wgcclber' target=\"_blank\">https://wandb.ai/mzekhov/cluster-search/runs/wgcclber</a><br/> View project at: <a href='https://wandb.ai/mzekhov/cluster-search' target=\"_blank\">https://wandb.ai/mzekhov/cluster-search</a><br/>Synced 5 W&B file(s), 2 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240816_223412-wgcclber/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No wandb run found.\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()\n",
    "\n",
    "wandb_run = wandb.run\n",
    "if wandb_run:\n",
    "    logged_metrics = wandb_run.history()\n",
    "    print(\"Logged Metrics:\")\n",
    "    for key, value in logged_metrics.items():\n",
    "        print(key, \":\", value)\n",
    "else:\n",
    "    print(\"No wandb run found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation\n",
    "\n",
    "model_name, model = selected_models[1]\n",
    "segmentation.create_segmentation_plots(\n",
    "    model,\n",
    "    model_name,\n",
    "    optimizer_name=optimizer_name\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
