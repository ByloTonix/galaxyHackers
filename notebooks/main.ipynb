{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import timm\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import torch_optimizer as optimizer\n",
    "import wandb\n",
    "\n",
    "from config import settings\n",
    "\n",
    "import models.spinalnet_resnet as spinalnet_resnet\n",
    "import models.effnet as effnet\n",
    "import models.densenet as densenet\n",
    "import models.spinalnet_vgg as spinalnet_vgg\n",
    "import models.vitL16 as vitL16\n",
    "import models.alexnet_vgg as alexnet_vgg\n",
    "import models.resnet18 as resnet18\n",
    "\n",
    "import  data\n",
    "# import data.segmentation as segmentation\n",
    "# import metrics.metrics as metrics\n",
    "from data import DataPart\n",
    "from train import Trainer\n",
    "import metrics\n",
    "\n",
    "\n",
    "all_models = [\n",
    "    ('ResNet18', resnet18.load_model()),\n",
    "    ('EfficientNet', effnet.load_model()),\n",
    "    # ('DenseNet', densenet.load_model()),\n",
    "    # ('SpinalNet_ResNet', spinalnet_resnet.load_model()),\n",
    "    # ('SpinalNet_VGG', spinalnet_vgg.load_model()),\n",
    "    # ('ViTL16', vitL16.load_model()),\n",
    "    # ('AlexNet_VGG', alexnet_vgg.load_model())\n",
    "]\n",
    "\n",
    "all_optimizers = [\n",
    "    ('SGD', optim.SGD),\n",
    "    ('Rprop', optim.Rprop),\n",
    "    ('Adam', optim.Adam),\n",
    "    ('NAdam', optim.NAdam),\n",
    "    ('RAdam', optim.RAdam),\n",
    "    ('AdamW', optim.AdamW),\n",
    "    #('Adagrad', optim.Adagrad),\n",
    "    ('RMSprop', optim.RMSprop),\n",
    "    #('Adadelta', optim.Adadelta),\n",
    "    ('DiffGrad', optimizer.DiffGrad),\n",
    "    # ('LBFGS', optim.LBFGS)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mszekhov/Desktop/current_projects/galaxyHackers/venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "252\n",
      "84\n",
      "84\n",
      "244\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "datasets, dataloaders = data.create_dataloaders()\n",
    "\n",
    "train_loader = dataloaders[DataPart.TRAIN]\n",
    "val_loader = dataloaders[DataPart.VALIDATE]\n",
    "test_loader = dataloaders[DataPart.TEST_DR5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parser = argparse.ArgumentParser(description='Model training')\n",
    "# parser.add_argument('--models', nargs='+', default=['ResNet18', 'EfficientNet', 'DenseNet', 'SpinalNet_ResNet', 'SpinalNet_VGG', 'ViTL16', 'AlexNet_VGG'],\n",
    "#                     help='List of models to train (default: all)')\n",
    "# parser.add_argument('--epochs', type=int, default=5, help='Number of epochs to train (default: 5)')\n",
    "# parser.add_argument('--lr', type=float, default=0.0001, help='Learning rate for optimizer (default: 0.0001)')\n",
    "# parser.add_argument('--mm', type=float, default=0.9, help='Momentum for optimizer (default: 0.9)')\n",
    "# parser.add_argument('--optimizer', choices=[name for name, _ in all_optimizers], default='Adam', help='Optimizer to use (default: Adam)')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# selected_models = [(model_name, model) for model_name, model in models if model_name in args.models]\n",
    "\n",
    "# num_epochs = args.epochs\n",
    "# lr = args.lr\n",
    "# momentum = args.mm\n",
    "# optimizer_name = args.optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_models = all_models[:2]\n",
    "\n",
    "num_epochs = 1\n",
    "lr = 0.0001\n",
    "momentum = 0.9\n",
    "optimizer_name = \"Adam\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzehov1\u001b[0m (\u001b[33mmzekhov\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/mszekhov/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mszekhov/Desktop/current_projects/galaxyHackers/wandb/run-20240816_223412-wgcclber</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mzekhov/cluster-search/runs/wgcclber' target=\"_blank\">blooming-mountain-47</a></strong> to <a href='https://wandb.ai/mzekhov/cluster-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mzekhov/cluster-search' target=\"_blank\">https://wandb.ai/mzekhov/cluster-search</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mzekhov/cluster-search/runs/wgcclber' target=\"_blank\">https://wandb.ai/mzekhov/cluster-search/runs/wgcclber</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if settings.wandb_api_token:\n",
    "    wandb.login(key=settings.wandb_api_token)\n",
    "    wandb.init(project='cluster-search', config={}, reinit=True)\n",
    "else:\n",
    "    wandb.init(project='cluster-search', config={}, reinit=True)\n",
    "\n",
    "\n",
    "wandb.config.models = [name for name, _ in selected_models]\n",
    "wandb.config.num_epochs = num_epochs\n",
    "wandb.config.lr = lr\n",
    "wandb.config.momentum = momentum\n",
    "wandb.config.optimizer = optimizer_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "results = {}\n",
    "val_results = {}\n",
    "\n",
    "classes = ('random', 'clusters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [02:09<00:00,  8.10s/batch]    | 0/1 [00:00<?, ?epoch/s]\n",
      "100%|██████████| 16/16 [00:50<00:00,  3.16s/it]\n",
      "100%|██████████| 16/16 [00:51<00:00,  3.21s/it]                                         \n",
      " 69%|██████▉   | 11/16 [07:45<03:31, 42.33s/batch]        | 0/1 [00:00<?, ?epoch/s]\n",
      "                                                                                   \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m      7\u001b[0m      optimizer \u001b[38;5;241m=\u001b[39m optimizer_class(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     10\u001b[0m      model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     11\u001b[0m      criterion\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trainer\u001b[38;5;241m.\u001b[39mglobal_step):\n\u001b[1;32m     21\u001b[0m      wandb\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m     22\u001b[0m           {\n\u001b[1;32m     23\u001b[0m                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_train_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: trainer\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m][step], \n\u001b[1;32m     24\u001b[0m                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_train_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m:trainer\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m][step], \n\u001b[1;32m     25\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_step\u001b[39m\u001b[38;5;124m'\u001b[39m: step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m})\n",
      "File \u001b[0;32m~/Desktop/current_projects/galaxyHackers/train.py:96\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m     94\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     95\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 96\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_train_batch()\n",
      "File \u001b[0;32m~/Desktop/current_projects/galaxyHackers/venv/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/current_projects/galaxyHackers/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/current_projects/galaxyHackers/venv/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for model_name, model in selected_models:\n",
    "     optimizer_class = dict(all_optimizers)[optimizer_name]\n",
    "\n",
    "     if optimizer_name in ['SGD', 'RMSprop']:\n",
    "          optimizer = optimizer_class(model.parameters(), lr=lr, momentum=momentum) \n",
    "     else:\n",
    "          optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "         \n",
    "     trainer = Trainer(\n",
    "          model=model,\n",
    "          criterion=criterion,\n",
    "          optimizer=optimizer,\n",
    "          train_dataloader=train_loader,\n",
    "          val_dataloader=val_loader,\n",
    "\n",
    "     )\n",
    "\n",
    "     trainer.train(num_epochs)\n",
    "\n",
    "     for step in range(trainer.global_step):\n",
    "          wandb.log(\n",
    "               {\n",
    "                    f'{model_name}_{optimizer_name}_train_loss': trainer.history['train_loss'][step], \n",
    "                    f'{model_name}_{optimizer_name}_train_accuracy':trainer.history['train_acc'][step], \n",
    "                    'global_step': step + 1})\n",
    "          \n",
    "     for epoch in range(num_epochs):\n",
    "          wandb.log(\n",
    "               {\n",
    "                    f'{model_name}_{optimizer_name}_val_loss': trainer.history['val_loss'][epoch], \n",
    "                    f'{model_name}_{optimizer_name}_val_accuracy': trainer.history['val_acc'][epoch], \n",
    "                    'epoch': epoch})\n",
    "\n",
    "     \n",
    "     train_table = wandb.Table(\n",
    "          data=[\n",
    "               [\n",
    "                    step, \n",
    "                    trainer.history['train_loss'][step], \n",
    "                    trainer.history['train_acc'][step]\n",
    "               ] for step in range(trainer.global_step)],\n",
    "          columns=[\"Epoch\", \"Loss\", \"Accuracy\"])\n",
    "\n",
    "     val_table = wandb.Table(\n",
    "          data=[\n",
    "               [\n",
    "                    epoch, \n",
    "                    trainer.history['val_loss'][epoch], \n",
    "                    trainer.history['val_acc'][epoch]\n",
    "               ] for epoch in range(num_epochs)],\n",
    "          columns=[\"Epoch\", \"Loss\", \"Accuracy\"])\n",
    "\n",
    "     wandb.log({\"Train Metrics\": train_table, \"Validation Metrics\": val_table})\n",
    "\n",
    "     y_pred, y_probs, y_true, *_ = trainer.test(test_loader)\n",
    "\n",
    "     metrics.modelPerformance(model_name, optimizer_name, y_true, y_pred, y_probs, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a965685e834803a28208ffff926c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.009 MB uploaded\\r'), FloatProgress(value=0.4286352967475131, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>ResNet18_Adam_train_accuracy</td><td>▆▆▅▆▆▆▇▇▇█▇▇▇▇█▁</td></tr><tr><td>ResNet18_Adam_train_loss</td><td>█▇█▇▇▇▅▅▅▄▆▅▃▂▁▇</td></tr><tr><td>ResNet18_Adam_val_accuracy</td><td>▁</td></tr><tr><td>ResNet18_Adam_val_loss</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>global_step</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>ResNet18_Adam_train_accuracy</td><td>0.0</td></tr><tr><td>ResNet18_Adam_train_loss</td><td>0.6936</td></tr><tr><td>ResNet18_Adam_val_accuracy</td><td>0.72852</td></tr><tr><td>ResNet18_Adam_val_loss</td><td>0.62251</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>global_step</td><td>16</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">blooming-mountain-47</strong> at: <a href='https://wandb.ai/mzekhov/cluster-search/runs/wgcclber' target=\"_blank\">https://wandb.ai/mzekhov/cluster-search/runs/wgcclber</a><br/> View project at: <a href='https://wandb.ai/mzekhov/cluster-search' target=\"_blank\">https://wandb.ai/mzekhov/cluster-search</a><br/>Synced 5 W&B file(s), 2 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240816_223412-wgcclber/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No wandb run found.\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()\n",
    "\n",
    "wandb_run = wandb.run\n",
    "if wandb_run:\n",
    "    logged_metrics = wandb_run.history()\n",
    "    print(\"Logged Metrics:\")\n",
    "    for key, value in logged_metrics.items():\n",
    "        print(key, \":\", value)\n",
    "else:\n",
    "    print(\"No wandb run found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mszekhov/Desktop/current_projects/galaxyHackers/segmentation.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model = torch.load(weights_path, map_location=device)\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:19<00:00,  2.79s/it]\n",
      "100%|██████████| 7/7 [00:20<00:00,  2.98s/it]\n",
      "100%|██████████| 7/7 [00:21<00:00,  3.12s/it]\n",
      "100%|██████████| 7/7 [00:20<00:00,  2.91s/it]\n",
      "100%|██████████| 7/7 [00:19<00:00,  2.85s/it]\n",
      "100%|██████████| 7/7 [00:20<00:00,  2.98s/it]\n",
      "100%|██████████| 7/7 [00:19<00:00,  2.81s/it]\n",
      "100%|██████████| 7/7 [00:20<00:00,  2.90s/it]\n",
      "100%|██████████| 7/7 [00:22<00:00,  3.25s/it]\n",
      "100%|██████████| 7/7 [00:22<00:00,  3.23s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:18<00:00,  2.66s/it]\n",
      "100%|██████████| 7/7 [00:19<00:00,  2.84s/it]\n",
      "100%|██████████| 7/7 [00:19<00:00,  2.78s/it]\n",
      "100%|██████████| 7/7 [00:28<00:00,  4.08s/it]\n",
      "100%|██████████| 7/7 [00:18<00:00,  2.62s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "23\n",
      "23\n",
      "23\n",
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:18<00:00,  2.66s/it]\n",
      "100%|██████████| 7/7 [00:22<00:00,  3.25s/it]\n",
      "100%|██████████| 7/7 [00:24<00:00,  3.56s/it]\n",
      "100%|██████████| 7/7 [00:25<00:00,  3.66s/it]\n",
      "100%|██████████| 7/7 [00:26<00:00,  3.74s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:50<00:00,  3.13s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 34.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "Failed attempt 0 to download /Users/mszekhov/Desktop/current_projects/galaxyHackers/storage/segmentation/samples/dr5_big/758/679.fits with an HTTPError\n",
      "Failed attempt 0 to download /Users/mszekhov/Desktop/current_projects/galaxyHackers/storage/segmentation/samples/dr5_big/758/728.fits with an HTTPError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:43<00:00,  2.74s/it]\n"
     ]
    }
   ],
   "source": [
    "import segmentation\n",
    "\n",
    "model_name, model = selected_models[0]\n",
    "segmentation.create_segmentation_plots(\n",
    "    model,\n",
    "    model_name,\n",
    "    optimizer_name=optimizer_name\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
